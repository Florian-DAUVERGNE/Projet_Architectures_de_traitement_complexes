{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet d'Analyse de Données de Capteurs pour la Classification d'Exercices\n",
    "\n",
    "## Description du Projet\n",
    "Ce projet vise à analyser des données provenant de multiples capteurs (EMG, IMU, IPS, COP, MOCAP) pour classifier différents types d'exercices physiques. Nous utilisons des approches de deep learning, notamment des architectures CNN-LSTM, pour effectuer cette classification.\n",
    "\n",
    "## Structure du Projet\n",
    "\n",
    "### 1. Préparation des Données\n",
    "- Extraction des données depuis des fichiers ZIP\n",
    "- Standardisation et nettoyage des données\n",
    "- Organisation des données par capteurs et type d'exercice\n",
    "\n",
    "### 2. Prétraitement\n",
    "- Rééchantillonnage des données à 60Hz\n",
    "- Normalisation des données\n",
    "- Création de fenêtres glissantes pour l'analyse temporelle\n",
    "\n",
    "### 3. Modélisation\n",
    "Nous comparons deux approches :\n",
    "- Un modèle CNN-LSTM simple\n",
    "- Un modèle CNN-LSTM multimodal fusionnant les données de tous les capteurs\n",
    "\n",
    "### 4. Validation\n",
    "Deux stratégies de validation sont utilisées :\n",
    "- Split normal (80-20)\n",
    "- Leave-One-Group-Out (LOGO) pour une validation plus robuste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des Bibliothèques\n",
    "Les bibliothèques principales utilisées dans ce projet :\n",
    "- numpy : pour les calculs numériques\n",
    "- pandas : pour la manipulation des données\n",
    "- matplotlib : pour la visualisation\n",
    "- sklearn : pour le prétraitement et l'évaluation\n",
    "- tensorflow/keras : pour les modèles de deep learning\n",
    "- imblearn : pour la gestion du déséquilibre des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Concatenate,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Input,\n",
    "    MaxPooling1D,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Préparation des Données\n",
    "\n",
    "### 2.1 Extraction des Fichiers\n",
    "Notre première étape consiste à extraire les données brutes depuis un fichier ZIP. Ces données sont ensuite organisées de manière structurée par capteur et par type d'exercice. Nous mettons en place un système de filtrage pour exclure les fichiers système et autres éléments non pertinents pour notre analyse.\n",
    "\n",
    "### 2.2 Transposition des Données\n",
    "Les données brutes nécessitent une restructuration pour être exploitables. Nous effectuons une lecture systématique des fichiers CSV, suivie d'une transposition pour obtenir un format adapté à notre analyse. Cette étape inclut également une standardisation des noms de colonnes pour assurer la cohérence des données.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_data(input_file):\n",
    "    # Charge le fichier CSV dans un DataFrame pandas\n",
    "    data = pd.read_csv(input_file)\n",
    "    \n",
    "    # Identifie les colonnes qui ne sont pas numériques ou des chiffres, et les considère comme des clés\n",
    "    non_numeric_keys = [key for key in data.keys() if not (isinstance(key, (int, float)) or key.isdigit())]\n",
    "    \n",
    "    # Renomme la première clé non numérique en 'ID' (clé primaire pour la transposition)\n",
    "    data = data.rename(columns={non_numeric_keys[0]: 'ID'})\n",
    "    \n",
    "    # Définit la colonne 'ID' comme index, puis transpose les données\n",
    "    data = data.set_index('ID').transpose()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def unzip_and_process_files(zip_file_path, extract_to_folder, ignore_patterns=None):    \n",
    "    # Vérifie si le fichier ZIP existe\n",
    "    if not os.path.exists(zip_file_path):\n",
    "        print(f\"Le fichier zip {zip_file_path} n'existe pas.\")\n",
    "        return\n",
    "    \n",
    "    # Vérifie si les fichiers sont déjà extraits dans le dossier de destination\n",
    "    if os.path.exists(extract_to_folder):\n",
    "        print(f\"Le fichier est déjà décompressé dans {extract_to_folder}.\")\n",
    "        return\n",
    "    \n",
    "    # Crée le dossier de destination si il n'existe pas\n",
    "    os.makedirs(extract_to_folder, exist_ok=True)\n",
    "\n",
    "    # Liste des fichiers à ignorer (par défaut, rien à ignorer)\n",
    "    if ignore_patterns is None:\n",
    "        ignore_patterns = []\n",
    "    \n",
    "    try:\n",
    "        # Ouvre le fichier ZIP pour extraction\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "            # Liste tous les fichiers présents dans l'archive ZIP\n",
    "            zip_files = zip_ref.namelist()\n",
    "\n",
    "            # Extraire uniquement les fichiers qui ne sont pas dans les fichiers à ignorer\n",
    "            for file_name in zip_files:\n",
    "                # Si le fichier correspond à un des patterns à ignorer, on l'ignore\n",
    "                if any(pattern in file_name for pattern in ignore_patterns):\n",
    "                    continue  # Ignore ce fichier et passe au suivant\n",
    "                \n",
    "                # Si le fichier est un CSV, on applique la transposition\n",
    "                if file_name.endswith('.csv'):\n",
    "                    # Extrait le fichier CSV dans le dossier spécifié\n",
    "                    zip_ref.extract(file_name, extract_to_folder)\n",
    "                    \n",
    "                    # Applique la transposition au fichier CSV extrait\n",
    "                    file_path = os.path.join(extract_to_folder, file_name)\n",
    "                    transposed_data = transpose_data(file_path)\n",
    "                    \n",
    "                    # Sauvegarde le fichier CSV transposé dans le même emplacement\n",
    "                    transposed_data.to_csv(file_path, index=False)\n",
    "                    print(f\"Fichier transposé et extrait : {file_path}\")\n",
    "            \n",
    "            # Message indiquant que l'extraction est terminée\n",
    "            print(f\"Extraction terminée dans {extract_to_folder}\")\n",
    "    \n",
    "    # Gestion des erreurs si le fichier ZIP est invalide\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"Erreur : Le fichier {zip_file_path} n'est pas un fichier zip valide.\")\n",
    "    except Exception as e:\n",
    "        # Capture d'autres erreurs\n",
    "        print(f\"Erreur lors de la décompression : {e}\")\n",
    "\n",
    "# Paramètres du fichier ZIP et du dossier de destination\n",
    "zip_file_path = 'dataset.zip'\n",
    "extract_to_folder = 'unprocessed_datasets' \n",
    "\n",
    "# Liste des fichiers à ignorer pendant l'extraction\n",
    "ignore_patterns = ['.txt', '_MACOSX', '.DS_Store']\n",
    "\n",
    "# Appel de la fonction pour décompresser et traiter les fichiers\n",
    "unzip_and_process_files(zip_file_path, extract_to_folder, ignore_patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Structure des Données\n",
    "\n",
    "Notre jeu de données comprend des enregistrements de plusieurs patients réalisant différents types d'exercices. \n",
    "\n",
    "Les données sont collectées via divers capteurs, chacun ayant sa propre fréquence d'échantillonnage. \n",
    "\n",
    "Cette diversité des sources d'information enrichit notre analyse tout en présentant des défis d'harmonisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Énumérage des fichiers\n",
    "\n",
    "Nous faisons un parcours de tous les fichiers dans le dossier unprocessed_datasets et enregistrons les chemins de chaque fichier dans une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_number_from_path(path):\n",
    "    # Utiliser une expression régulière pour extraire le premier nombre dans le chemin\n",
    "    match = re.search(r'\\\\(\\d+)', path)  # Cherche un nombre qui suit un backslash\n",
    "    if match:\n",
    "        return int(match.group(1))  # Retourne le nombre trouvé en tant qu'entier\n",
    "    return 0  # Si aucun nombre n'est trouvé, on retourne 0\n",
    "\n",
    "def get_files_paths(directory_path):\n",
    "    files_paths = []  # Liste pour stocker les chemins des fichiers trouvés\n",
    "    \n",
    "    # Parcourir les fichiers dans le répertoire et ses sous-répertoires\n",
    "    for root, dir, files in os.walk(directory_path):\n",
    "        for file_name in files:\n",
    "            # Ajouter le chemin complet du fichier à la liste\n",
    "            files_paths.append(os.path.join(root, file_name))\n",
    "\n",
    "    return files_paths\n",
    "\n",
    "# Utiliser la fonction pour obtenir tous les chemins des fichiers dans 'unprocessed_datasets'\n",
    "# Trier les chemins de fichiers selon le premier nombre trouvé dans chaque chemin (en utilisant 'extract_number_from_path')\n",
    "UNPROCESSED_FILES_PATH = sorted(get_files_paths('unprocessed_datasets'), key=extract_number_from_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Extraction du Numéro de Patient\n",
    "\n",
    "Nous faisons un parcours de tous les fichiers dans le dossier unprocessed_datasets et enregistrons les numéros de patient dans un ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_patient(files_paths):\n",
    "    # Utiliser un ensemble (set) pour garder les numéros de patient uniques\n",
    "    patient_numbers = set()\n",
    "\n",
    "    # Parcourir les chemins de fichiers\n",
    "    for file_path in files_paths:\n",
    "        # Extraire le numéro de patient (le second élément du chemin)\n",
    "        # On suppose que le numéro de patient est situé dans la deuxième position du chemin\n",
    "        # par exemple : 'unprocessed_datasets\\\\1\\\\exercice\\\\sensor.csv'\n",
    "        patient_number = file_path.split('\\\\')[1]\n",
    "        \n",
    "        # Ajouter le numéro de patient à l'ensemble\n",
    "        # L'ensemble garantit l'unicité des numéros de patient\n",
    "        patient_numbers.add(patient_number)\n",
    "\n",
    "    # Retourner le nombre de patients uniques\n",
    "    # La longueur de l'ensemble correspond au nombre de patients distincts\n",
    "    return len(patient_numbers)\n",
    "\n",
    "# Appel de la fonction pour obtenir le nombre de patients uniques à partir des chemins de fichiers\n",
    "SAMPLE_SIZE = get_number_of_patient(UNPROCESSED_FILES_PATH)\n",
    "\n",
    "# Afficher le nombre de patients\n",
    "print(f\"Nombre de patients : {SAMPLE_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Extraction du Type d'Exercice\n",
    "\n",
    "Nous faisons un parcours de tous les fichiers dans le dossier unprocessed_datasets et enregistrons les types d'exercices dans un ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exercice_type(files_paths):\n",
    "    # Créer un ensemble pour stocker les types d'exercices uniques\n",
    "    exercice_type = set()\n",
    "\n",
    "    # Parcourir les chemins de fichiers\n",
    "    for file_path in files_paths:\n",
    "        # Extraire le numéro du patient à partir du chemin\n",
    "        patient_number = file_path.split('\\\\')[1]\n",
    "        \n",
    "        # Si le numéro du patient n'est pas '1', on arrête la boucle\n",
    "        if patient_number != '1':\n",
    "            break \n",
    "        \n",
    "        # Extraire le nom de l'exercice, qui se trouve dans le troisième élément du chemin\n",
    "        exercice_name = file_path.split('\\\\')[2]\n",
    "        \n",
    "        # Ajouter le nom de l'exercice à l'ensemble\n",
    "        exercice_type.add(exercice_name)\n",
    "\n",
    "    # Retourner l'ensemble des types d'exercices uniques\n",
    "    return exercice_type\n",
    "\n",
    "# Obtenir les types d'exercices pour le patient '1' à partir de la liste des chemins de fichiers\n",
    "EXERCICES_TYPE = get_exercice_type(UNPROCESSED_FILES_PATH)\n",
    "\n",
    "# Afficher les types d'exercices extraits\n",
    "print(f\"Types d'exercices : {EXERCICES_TYPE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Extraction du Type de Capteur et de la Fréquence d'Échantillonnage\n",
    "\n",
    "Nous faisons un parcours de tous les fichiers dans le dossier unprocessed_datasets et enregistrons les types de capteurs dans un dictionnaire.\n",
    "\n",
    "De plus, nous estimons la fréquence d'échantillonnage du capteur à partir de la durée de l'exercice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_sampling_rate_from_duration(file_path, duration_seconds):\n",
    "    # Lire le fichier CSV\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Nombre d'échantillons dans le fichier\n",
    "    num_samples = data.shape[0]\n",
    "    \n",
    "    # Estimer la fréquence d'échantillonnage : nombre d'échantillons / durée en secondes\n",
    "    sampling_rate = round(num_samples / duration_seconds / 10) * 10\n",
    "    \n",
    "    # Retourner la fréquence d'échantillonnage estimée\n",
    "    return round(sampling_rate)\n",
    "\n",
    "def get_sensors_type_and_sampling_rate(files_paths):\n",
    "    # Dictionnaire pour stocker les types de capteurs et leurs fréquences d'échantillonnage\n",
    "    SENSORS_TYPE = {}\n",
    "    \n",
    "    # Durée des enregistrements, utilisée pour estimer la fréquence d'échantillonnage\n",
    "    DURATION = 66.1  # Durée en secondes, par exemple 66.1 secondes pour chaque fichier\n",
    "\n",
    "    # Parcourir les chemins des fichiers\n",
    "    for file_path in files_paths:\n",
    "        # Extraire le numéro du patient à partir du chemin\n",
    "        patient_number = file_path.split('\\\\')[1]\n",
    "        \n",
    "        # Si le numéro du patient n'est pas '1', ignorer ce fichier\n",
    "        if patient_number != '1':\n",
    "            break  # Si on a traité le patient '1', on arrête l'analyse des autres patients\n",
    "        \n",
    "        # Extraire le nom du capteur à partir du chemin du fichier\n",
    "        sensor_name = file_path.split('\\\\')[3].split('_')[0].replace('.csv', '')\n",
    "        \n",
    "        # Estimer la fréquence d'échantillonnage pour ce capteur\n",
    "        sensor_rate = estimate_sampling_rate_from_duration(file_path, DURATION)\n",
    "        \n",
    "        # Ajouter le capteur et sa fréquence d'échantillonnage au dictionnaire si ce capteur n'existe pas déjà\n",
    "        if sensor_name not in SENSORS_TYPE:\n",
    "            SENSORS_TYPE[sensor_name] = sensor_rate\n",
    "\n",
    "    # Retourner le dictionnaire des types de capteurs et leurs fréquences d'échantillonnage\n",
    "    return SENSORS_TYPE\n",
    "\n",
    "# Obtenir les types de capteurs et leurs fréquences d'échantillonnage\n",
    "SENSORS_TYPE = get_sensors_type_and_sampling_rate(UNPROCESSED_FILES_PATH)\n",
    "\n",
    "print(\"Types de capteurs détectés et leurs fréquences d'échantillonnage : \", SENSORS_TYPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prétraitement des Données\n",
    "\n",
    "### 3.1 Analyse Initiale des Données\n",
    "Notre première étape consiste en une analyse approfondie des données brutes. \n",
    "\n",
    "Cette exploration nous a permis d'identifier plusieurs caractéristiques importantes de notre jeu de données. Nous avons notamment constaté que les capteurs fonctionnent à des fréquences d'échantillonnage différentes, allant de 60Hz à 200Hz selon le type de capteur. \n",
    "\n",
    "Les enregistrements suivent une durée standardisée, bien que certains fichiers contiennent plusieurs sessions d'enregistrement, identifiées par des suffixes (_1, _2, _3).\n",
    "\n",
    "Nous avons également repéré la présence de valeurs manquantes et de bruit dans les signaux, nécessitant un traitement particulier.\n",
    "\n",
    "### 3.2 Nettoyage des Données\n",
    "Le nettoyage des données constitue une étape fondamentale de notre prétraitement.\n",
    "\n",
    "Nous commençons par fusionner les enregistrements multiples d'un même exercice pour obtenir des séquences complètes et cohérentes.\n",
    "\n",
    "Les fichiers temporaires et les doublons sont ensuite supprimés pour éviter toute redondance.\n",
    "\n",
    "Les valeurs aberrantes, identifiées par des analyses statistiques, sont éliminées, tandis que les valeurs manquantes sont comblées par interpolation linéaire pour maintenir la continuité des signaux.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parcours de tous les chemins de fichiers dans la liste UNPROCESSED_FILES_PATH\n",
    "for file_path in UNPROCESSED_FILES_PATH:\n",
    "    \n",
    "    # Si le fichier contient '_1' dans son nom\n",
    "    if('_1' in file_path):\n",
    "        # Lire le fichier CSV et sauvegarder sous un nouveau nom sans '_1'\n",
    "        pd.read_csv(file_path).to_csv(file_path.replace('_1',''), index=False)\n",
    "        \n",
    "        # Supprimer l'ancien fichier avec '_1' dans son nom\n",
    "        os.remove(file_path)\n",
    "        \n",
    "    # Si le fichier contient '_2' dans son nom\n",
    "    if('_2' in file_path):\n",
    "        # Lire les données de deux fichiers, l'un avec '_2' et l'autre sans '_2'\n",
    "        data_1 = pd.read_csv(file_path.replace('_2',''))  # Fichier sans '_2'\n",
    "        data_2 = pd.read_csv(file_path)  # Fichier avec '_2'\n",
    "        \n",
    "        # Fusionner les deux ensembles de données\n",
    "        pd.concat([data_1, data_2]).to_csv(file_path.replace('_2',''), index=False)\n",
    "        \n",
    "        # Supprimer l'ancien fichier avec '_2' dans son nom\n",
    "        os.remove(file_path)\n",
    "        \n",
    "    # Si le fichier contient '_3' dans son nom\n",
    "    if('_3' in file_path):\n",
    "        # Lire les données de deux fichiers, l'un avec '_3' et l'autre sans '_3'\n",
    "        data_1 = pd.read_csv(file_path.replace('_3',''))  # Fichier sans '_3'\n",
    "        data_2 = pd.read_csv(file_path)  # Fichier avec '_3'\n",
    "        \n",
    "        # Fusionner les deux ensembles de données\n",
    "        pd.concat([data_1, data_2]).to_csv(file_path.replace('_3',''), index=False)\n",
    "        \n",
    "        # Supprimer l'ancien fichier avec '_3' dans son nom\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous supprimons les suffixes '_1', '_2' et '_3' de la liste des fichiers et supprimons les doublons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_suffix_and_duplicates(file_names):   \n",
    "    # Liste qui contiendra les noms de fichiers nettoyés\n",
    "    cleaned_files = []\n",
    "    \n",
    "    # Set utilisé pour garder une trace des fichiers déjà rencontrés\n",
    "    seen = set()\n",
    "\n",
    "    # Ce pattern capture tout avant un underscore suivi d'un chiffre (_n) et l'extension du fichier\n",
    "    pattern = re.compile(r\"(.*?)(_\\d+)?\\.[^\\.]+$\")  # Exemple : \"file_1.csv\" => \"file\"\n",
    "    \n",
    "    for file in file_names:\n",
    "        # Appliquer l'expression régulière pour extraire la partie avant '_n' et l'extension\n",
    "        match = pattern.match(file)\n",
    "        \n",
    "        if match:\n",
    "            base_name = match.group(1)  # On récupère le nom de base sans le suffixe _n\n",
    "            if base_name not in seen:\n",
    "                # Si le fichier n'a pas encore été rencontré, on le garde dans la liste\n",
    "                cleaned_files.append(file.replace('_1', ''))  # Enlève le suffixe '_1'\n",
    "                seen.add(base_name)  # On ajoute le nom de base au set 'seen' pour éviter les doublons\n",
    "        else:\n",
    "            # Si le fichier ne correspond pas au pattern (ex. pas de suffixe '_n'), on l'ajoute tel quel\n",
    "            cleaned_files.append(file.replace('_1', ''))  # On enlève '_1' s'il est présent\n",
    "            seen.add(file)  # Ajout du fichier complet au set 'seen'\n",
    "\n",
    "    # Retourne la liste des fichiers nettoyés, sans doublons\n",
    "    return cleaned_files\n",
    "\n",
    "# Appliquer cette fonction à la liste des fichiers non traités\n",
    "UNPROCESSED_FILES_PATH = remove_suffix_and_duplicates(UNPROCESSED_FILES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Rééchantillonnage\n",
    "L'harmonisation des fréquences d'échantillonnage représente un défi majeur de notre prétraitement. \n",
    "\n",
    "Nous avons choisi de standardiser toutes nos données à 60Hz, correspondant à la fréquence la plus basse parmi nos capteurs.\n",
    "\n",
    "Cette transformation est réalisée par interpolation linéaire, permettant de préserver au mieux la dynamique des signaux.\n",
    "\n",
    "Une attention particulière est portée à la vérification de la cohérence temporelle après rééchantillonnage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resampling(dataset_wrong_frequency, dataset_desired_frequency, wrong_frequency, desired_frequency, debug=False):\n",
    "    # Si les fréquences sont déjà égales, on renvoie simplement le dataset original sans modification\n",
    "    if wrong_frequency == desired_frequency:\n",
    "        return dataset_wrong_frequency\n",
    "    \n",
    "    # Création d'un DataFrame avec une nouvelle colonne 'time' pour les données avec une fréquence incorrecte\n",
    "    df_wrong_Hz = dataset_wrong_frequency\n",
    "    sampling_rate_wrong_Hz = wrong_frequency\n",
    "    # Crée une série de temps basée sur la fréquence d'échantillonnage initiale (en microsecondes)\n",
    "    df_wrong_Hz['time'] = pd.date_range(start='00:00:00', periods=df_wrong_Hz.shape[0], freq=f'{int(1e6 / sampling_rate_wrong_Hz)}us')\n",
    "\n",
    "    # Nombre de points dans les données à la fréquence erronée\n",
    "    n_points_wrong_Hz = len(df_wrong_Hz)\n",
    "    \n",
    "    # Nombre de points dans les données à la fréquence désirée\n",
    "    n_points_desired_Hz = dataset_desired_frequency.shape[0]\n",
    "\n",
    "    # Crée une série de temps avec la fréquence désirée\n",
    "    time = pd.date_range(start=df_wrong_Hz['time'].iloc[0], periods=n_points_desired_Hz, freq=f'{int(desired_frequency/10)}ms')\n",
    "    \n",
    "    # Définit la colonne 'time' comme index\n",
    "    df_wrong_Hz.set_index('time', inplace=True)\n",
    "    \n",
    "    # Rééchantillonne les données à la fréquence désirée en réindexant et en interpolant les valeurs manquantes\n",
    "    df_wrong_Hz_resampled = df_wrong_Hz.reindex(time).interpolate(method='linear')\n",
    "\n",
    "    # Si le mode debug est activé, affiche des informations sur le rééchantillonnage\n",
    "    if debug == True:\n",
    "        print(f\"Nombre de points à {wrong_frequency} Hz avant interpolation:\", n_points_wrong_Hz)\n",
    "        print(f\"Nombre de points à {desired_frequency} Hz après interpolation:\", len(df_wrong_Hz_resampled))\n",
    "    \n",
    "    return df_wrong_Hz_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous créons un ensemble de sous-données.\n",
    "\n",
    "Chaque sous-donnée est stockée dans le dossier processed_datasets et est rééchantillonnée à 60Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dossier processed_datasets si ce n'est pas déjà fait\n",
    "if not os.path.exists('processed_datasets'):\n",
    "    os.makedirs('processed_datasets')\n",
    "\n",
    "# Processus de rééchantillonnage pour chaque fichier non traité\n",
    "for file_path in UNPROCESSED_FILES_PATH:\n",
    "    # Chemin vers le dossier des fichiers traités\n",
    "    PROCESSED_FILES_PATH = os.path.dirname(file_path.replace('unprocessed_datasets', 'processed_datasets'))\n",
    "    \n",
    "    # Si le dossier n'existe pas, le créer\n",
    "    if not os.path.exists(PROCESSED_FILES_PATH):\n",
    "        os.makedirs(PROCESSED_FILES_PATH)\n",
    "    \n",
    "    # Nom du capteur, extrait du chemin du fichier\n",
    "    sensor_name = file_path.split('\\\\')[3].split('_')[0].replace('.csv','')\n",
    "    \n",
    "    # Fréquence de mesure du capteur (définie dans un dictionnaire global SENSORS_TYPE)\n",
    "    sensor_rate = SENSORS_TYPE[sensor_name]\n",
    "    \n",
    "    # Si le fichier traité n'existe pas déjà\n",
    "    if not os.path.exists(f'{PROCESSED_FILES_PATH}/{sensor_name}.csv'):\n",
    "        # Applique le rééchantillonnage des données à 60 Hz (en utilisant un autre fichier comme référence)\n",
    "        processed_data = resampling(pd.read_csv(file_path), pd.read_csv('unprocessed_datasets\\\\1\\\\back\\\\cop.csv'), sensor_rate, 60, debug=True)\n",
    "        \n",
    "        # Sauvegarde les données rééchantillonnées dans le dossier des fichiers traités\n",
    "        processed_data.to_csv(f'{PROCESSED_FILES_PATH}/{sensor_name}.csv', index=False)\n",
    "        \n",
    "        # Affiche un message indiquant le rééchantillonnage\n",
    "        print(f'{PROCESSED_FILES_PATH}/{sensor_name}.csv est passé de {sensor_rate} Hz à 60Hz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Structuration des Données\n",
    "Notre approche de validation nécessite deux organisations distinctes des données.\n",
    "\n",
    "La première, que nous appelons \"Split Normal\", suit une répartition classique 80-20 entre données d'entraînement et de test. \n",
    "\n",
    "La seconde, plus rigoureuse, utilise une validation de type \"Leave-One-Group-Out\" (LOGO), où nous réservons les données de cinq patients pour le test, tandis que les vingt autres servent à l'entraînement. \n",
    "\n",
    "Cette double approche nous permettra d'évaluer la robustesse de nos modèles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous récupérons le chemin des fichiers traités et nous les ajoutons dans une liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des fichiers traités\n",
    "PROCESSED_FILES_PATH = sorted(get_files_paths('processed_datasets'), key=extract_number_from_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1 Normal Split\n",
    "\n",
    "Nous créons un ensemble de données, qui se répartit en 80% d'entraînement et 20% de test, à l'aide des fichiers traités.\n",
    "\n",
    "Nous créons un dossier normal_split si ce n'est pas déjà fait dans lequel nous créons un fichier pour chaque capteur.\n",
    "\n",
    "Ensuite, nous concaténons les données de chaque patient pour ce capteur et nous ajoutons une colonne label qui contient le type d'exercice.\n",
    "\n",
    "Enfin, nous sauvegardons les données dans le dossier normal_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si le dossier 'normal_split' existe déjà\n",
    "# Si ce n'est pas le cas, le créer\n",
    "if not os.path.exists('normal_split'):\n",
    "    os.makedirs('normal_split')  # Créer le dossier 'normal_split' si nécessaire\n",
    "\n",
    "# Boucle sur chaque type de capteur dans SENSORS_TYPE\n",
    "for sensor in SENSORS_TYPE:\n",
    "    # Vérifier si le fichier CSV pour ce capteur existe déjà dans le dossier 'normal_split'\n",
    "    if os.path.exists(f'normal_split/{sensor}.csv'):\n",
    "        print(f'normal_split/{sensor}.csv existe déjà')  # Afficher un message si le fichier existe\n",
    "        continue  # Passer au capteur suivant si le fichier existe déjà\n",
    "    \n",
    "    # Si le fichier n'existe pas, afficher un message et commencer sa création\n",
    "    print(f'normal_split/{sensor}.csv en cours de création')\n",
    "    \n",
    "    # Créer un DataFrame vide pour stocker les données\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Boucle sur chaque exercice dans EXERCICES_TYPE\n",
    "    for exercice in EXERCICES_TYPE:\n",
    "        # Boucle sur un intervalle de 1 à SAMPLE_SIZE pour parcourir les différents échantillons\n",
    "        for i in range(1, SAMPLE_SIZE + 1):\n",
    "            try:\n",
    "                # Lire le fichier CSV de données traité pour chaque exercice, sensor et échantillon\n",
    "                data = pd.read_csv(f'processed_datasets/{i}/{exercice}/{sensor}.csv')\n",
    "                data[\"label\"] = exercice  # Ajouter une colonne 'label' correspondant à l'exercice actuel\n",
    "                df = pd.concat([df, data], axis=0).reset_index(drop=True)  # Concaténer les données au DataFrame global\n",
    "            except:\n",
    "                continue  # Si une erreur se produit lors de la lecture du fichier, passer au fichier suivant\n",
    "\n",
    "        # Sauvegarder le DataFrame combiné dans un fichier CSV pour ce capteur\n",
    "        df.to_csv(f'normal_split/{sensor}.csv', index=False)\n",
    "\n",
    "    print(f'normal_split/{sensor}.csv créé')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 LOGO Split\n",
    "\n",
    "Nous créons un ensemble de données, qui se répartit en 20 patients pour l'entraînement et 5 patients pour le test, à l'aide des fichiers traités.\n",
    "\n",
    "Nous créons un dossier logo_split si ce n'est pas déjà fait dans lequel nous créons un fichier pour chaque capteur.\n",
    "\n",
    "Ensuite, nous concaténons les données de chaque patient pour ce capteur et nous ajoutons une colonne label qui contient le type d'exercice.\n",
    "\n",
    "Nous ajoutons également une colonne patient qui contient le numéro du patient.\n",
    "\n",
    "Enfin, nous sauvegardons les données dans le dossier logo_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si le dossier 'logo_split' existe déjà\n",
    "# Si ce n'est pas le cas, le créer\n",
    "if not os.path.exists('logo_split'):\n",
    "    os.makedirs('logo_split')  # Créer le dossier 'logo_split' si nécessaire\n",
    "\n",
    "# Boucle sur chaque type de capteur dans SENSORS_TYPE\n",
    "for sensor in SENSORS_TYPE:\n",
    "    # Vérifier si le fichier CSV pour ce capteur existe déjà dans le dossier 'logo_split'\n",
    "    if os.path.exists(f'logo_split/{sensor}.csv'):\n",
    "        print(f'logo_split/{sensor}.csv existe déjà')  # Afficher un message si le fichier existe\n",
    "        continue  # Passer au capteur suivant si le fichier existe déjà\n",
    "    \n",
    "    # Si le fichier n'existe pas, afficher un message et commencer sa création\n",
    "    print(f'logo_split/{sensor}.csv en cours de création')\n",
    "    \n",
    "    # Créer un DataFrame vide pour stocker les données\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Boucle sur chaque exercice dans EXERCICES_TYPE\n",
    "    for exercice in EXERCICES_TYPE:\n",
    "        # Boucle sur un intervalle de 1 à SAMPLE_SIZE pour parcourir les différents échantillons\n",
    "        for i in range(1, SAMPLE_SIZE + 1):\n",
    "            try:\n",
    "                # Lire le fichier CSV de données traité pour chaque exercice, sensor et échantillon\n",
    "                data = pd.read_csv(f'processed_datasets/{i}/{exercice}/{sensor}.csv')\n",
    "                data[\"label\"] = exercice  # Ajouter une colonne 'label' correspondant à l'exercice actuel\n",
    "                data[\"patient\"] = i  # Ajouter une colonne 'patient' pour l'échantillon actuel (numéro d'échantillon)\n",
    "                df = pd.concat([df, data], axis=0).reset_index(drop=True)  # Concaténer les données au DataFrame global\n",
    "            except:\n",
    "                continue  # Si une erreur se produit lors de la lecture du fichier, passer au fichier suivant\n",
    "\n",
    "        # Sauvegarder le DataFrame combiné dans un fichier CSV pour ce capteur\n",
    "        df.to_csv(f'logo_split/{sensor}.csv', index=False)\n",
    "\n",
    "    print(f'logo_split/{sensor}.csv créé')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Gestion de l'Équilibrage des Classes\n",
    "\n",
    "L'équilibrage des classes représente un aspect crucial de notre prétraitement.\n",
    "\n",
    "#### 3.5.1 Vérification des classes des datasets\n",
    "Nous créons un tableau pour le nombre de mesures par exercice et par capteur pour le Normal Split et le LOGO Split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de DataFrames vides pour stocker les résultats\n",
    "# Les colonnes correspondent aux exercices et les index aux types de capteurs\n",
    "number_of_measures_normal = pd.DataFrame(columns=list(EXERCICES_TYPE), index=list(SENSORS_TYPE.keys()))\n",
    "number_of_measures_logo = pd.DataFrame(columns=list(EXERCICES_TYPE), index=list(SENSORS_TYPE.keys()))\n",
    "\n",
    "# Boucle pour chaque exercice et chaque capteur\n",
    "for exercice in EXERCICES_TYPE:\n",
    "    for sensor in SENSORS_TYPE:\n",
    "        # Charger les données des fichiers CSV respectifs pour 'normal_split' et 'logo_split'\n",
    "        normal_data = pd.read_csv(f\"normal_split/{sensor}.csv\")\n",
    "        logo_data = pd.read_csv(f\"logo_split/{sensor}.csv\")\n",
    "        \n",
    "        # Calculer le nombre de mesures pour chaque exercice dans 'normal_split'\n",
    "        # Filtrer les données en fonction de l'exercice et compter les lignes\n",
    "        number_of_measures_normal.loc[sensor, exercice] = normal_data[normal_data['label'] == exercice].shape[0]\n",
    "        \n",
    "        # Calculer le nombre de mesures pour chaque exercice dans 'logo_split'\n",
    "        # Idem pour le jeu de données 'logo_split'\n",
    "        number_of_measures_logo.loc[sensor, exercice] = logo_data[logo_data['label'] == exercice].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puis nous affichons le nombre de mesures par exercice et par capteur pour le Normal Split et le LOGO Split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres des graphiques\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))  # 1 ligne, 2 colonnes\n",
    "\n",
    "# Graphique 1: Mesures Normal\n",
    "number_of_measures_normal.plot(kind='bar', ax=ax1, width=0.8, color=['skyblue', 'lightgreen', 'coral', 'violet'])\n",
    "ax1.set_xlabel(\"Type de capteur\", fontsize=12)\n",
    "ax1.set_ylabel(\"Nombre de mesures\", fontsize=12)\n",
    "ax1.set_title(\"Répartition des données Normal Split\", fontsize=14)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.legend(title=\"Type d'exercice\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "# Graphique 2: Mesures LOGO\n",
    "number_of_measures_normal.plot(kind='bar', ax=ax2, width=0.8, color=['skyblue', 'lightgreen', 'coral', 'violet'])\n",
    "ax2.set_xlabel(\"Type de capteur\", fontsize=12)\n",
    "ax2.set_ylabel(\"Nombre de mesures\", fontsize=12)\n",
    "ax2.set_title(\"Répartition des données LOGO Split\", fontsize=14)\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.legend(title=\"Type d'exercice\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "# Ajuster les espacements pour éviter le chevauchement\n",
    "plt.tight_layout()\n",
    "\n",
    "# Afficher les graphiques\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons alors remarquer des déséquilibres dans la représentation des différents exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5.2 Application de l'Équilibrage des Classes\n",
    "Notre analyse initiale ayant révélé des déséquilibres dans la représentation des différents exercices, nous utilisons une combinaison de techniques de sur-échantillonnage (SMOTE) et de sous-échantillonnage pour obtenir des classes équilibrées. \n",
    "\n",
    "Cette étape est particulièrement importante pour éviter les biais dans l'apprentissage de nos modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous récupérons le chemin des différents datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupérer et trier les fichiers dans le répertoire 'normal_split' et 'logo_split' \n",
    "NORMAL_SPLIT_FILES_PATH = sorted(get_files_paths('normal_split'), key=extract_number_from_path)\n",
    "LOGO_SPLIT_FILES_PATH = sorted(get_files_paths('logo_split'), key=extract_number_from_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_balance(y, target_count=100000):\n",
    "    \"\"\"Vérifie si la distribution des classes est équilibrée après rééchantillonnage\"\"\"\n",
    "    class_counts = y.value_counts()\n",
    "    balanced = all(count == target_count for count in class_counts)\n",
    "    if not balanced:\n",
    "        print(f\"Classes are not balanced. Current class distribution: {class_counts}\")\n",
    "    return balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous commençons par charger les fichiers CSV à partir du chemin spécifié.\n",
    "\n",
    "Ensuite nous supprimons les lignes contenant des valeurs manquantes. \n",
    "\n",
    "Nous séparons les caractéristiques (X) des labels (y) pour analyser la distribution des classes dans les données.\n",
    "\n",
    "Si la taille du jeu de données est inférieure à 400 000 échantillons, nous procédons à un rééchantillonnage pour équilibrer les classes. \n",
    "\n",
    "Si une classe est sur-représentée (plus de 100 000 échantillons), nous appliquons un sous-échantillonnage aléatoire (Random Under-Sampling) pour réduire la taille des classes les plus présentes. \n",
    "\n",
    "Si toutes les classes sont sous-représentées ou équilibrées, nous utilisons SMOTE (Synthetic Minority Over-sampling Technique) pour générer des exemples synthétiques et équilibrer les classes. \n",
    "\n",
    "Après chaque rééchantillonnage, nous vérifions si l'équilibre des classes est atteint. \n",
    "\n",
    "Si les classes sont équilibrées, nous sauvegardons les données rééchantillonnées dans le fichier original, sinon nous continuons avec une nouvelle tentative. \n",
    "\n",
    "Ce processus est répété pour chaque fichier de données spécifié dans la liste NORMAL_SPLIT_FILES_PATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_and_check(file_path, max_attempts=3):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        # Séparation des caractéristiques et des labels\n",
    "        X = df.drop(columns=['label'])\n",
    "        y = df['label']\n",
    "        \n",
    "        # Vérification de la distribution des classes actuelles\n",
    "        current_class_counts = y.value_counts()\n",
    "        print(f\"Initial class distribution for {file_path}: {current_class_counts}\")\n",
    "        if X.shape[0] != 400000:\n",
    "            print(f\"Before SMOTE or Under-Sampling to {file_path}, attempt {attempts + 1}\")\n",
    "            print(X.shape, y.shape)\n",
    "            \n",
    "            # Appliquer un RandomUnderSampler uniquement si une classe a plus d'échantillons que la cible\n",
    "            if any(current_class_counts[exercice] > 100000 for exercice in EXERCICES_TYPE):\n",
    "                current_class_counts[exercice]\n",
    "                print(\"Applying Random Under-Sampling due to over-represented classes\")\n",
    "                sampling_strategy = {exercice: min(100000, current_class_counts[exercice]) for exercice in EXERCICES_TYPE}\n",
    "                under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "                X_resampled, y_resampled = under_sampler.fit_resample(X, y)\n",
    "                print(f\"After under-sampling, class distribution: {pd.Series(y_resampled).value_counts()}\")\n",
    "                balanced_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                                        pd.DataFrame({'label': y_resampled})], axis=1)\n",
    "                # Sauvegarder le DataFrame rééchantillonné dans un nouveau fichier CSV\n",
    "                df = balanced_df\n",
    "            else:\n",
    "                # Appliquer SMOTE en ajustant la stratégie de rééchantillonnage\n",
    "                sampling_strategy = {exercice: 100000 for exercice in EXERCICES_TYPE}\n",
    "                smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "                balanced_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                        pd.DataFrame({'label': y_resampled})], axis=1)\n",
    "                # Sauvegarder le DataFrame rééchantillonné dans un nouveau fichier CSV\n",
    "                df = balanced_df\n",
    "                print(f\"After SMOTE, class distribution: {pd.Series(y_resampled).value_counts()}\")\n",
    "\n",
    "            # Vérification de l'équilibre des classes après rééchantillonnage\n",
    "            if check_class_balance(y_resampled):\n",
    "                balanced_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                                        pd.DataFrame({'label': y_resampled})], axis=1)\n",
    "                # Sauvegarder le DataFrame rééchantillonné dans un nouveau fichier CSV\n",
    "                balanced_df.to_csv(file_path, index=False)\n",
    "                print(f\"Successfully balanced and saved {file_path}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Warning: Classes are still not balanced in {file_path} after attempt {attempts + 1}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"Already balanced {file_path}\")\n",
    "            break\n",
    "\n",
    "        attempts += 1\n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"Max attempts reached for {file_path}. Consider adjusting strategy or target count.\")\n",
    "\n",
    "for file_path in NORMAL_SPLIT_FILES_PATH:\n",
    "    resample_and_check(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appliquons ensuite le même procédé pour chaque fichier de données spécifié dans la liste LOGO_SPLIT_FILES_PATH.\n",
    "\n",
    "Mais, il existe une différence entre les données LOGO et Normal.\n",
    "\n",
    "Les données LOGO possède un champ contenant le numéro du patient ce qui va modifier la manière de gérer l'équilibrage des données.\n",
    "\n",
    "En effet, il faut séparer le numéro du patient lors de la création des nouvelles données afin de ne générer que des données de capteurs.\n",
    "\n",
    "Nous attribuons par la suite une numéro de patient aléatoire au données générées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_and_check_LOGO(file_path, max_attempts=3):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna(inplace=True)\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        X = df.drop(columns=['label','patient'])  # Caractéristiques\n",
    "        y = df['label']  # Labels\n",
    "        z = df['patient']\n",
    "        \n",
    "        # Vérification de la distribution des classes actuelles\n",
    "        current_class_counts = y.value_counts()\n",
    "        print(f\"Initial class distribution for {file_path}: {current_class_counts}\")\n",
    "        if X.shape[0] != 400000:\n",
    "            print(f\"Before SMOTE or Under-Sampling to {file_path}, attempt {attempts + 1}\")\n",
    "            print(X.shape, y.shape)\n",
    "            \n",
    "            # Appliquer un RandomUnderSampler uniquement si une classe a plus d'échantillons que la cible\n",
    "            if any(current_class_counts[exercice] > 100000 for exercice in EXERCICES_TYPE):\n",
    "                print(\"Applying Random Under-Sampling due to over-represented classes\")\n",
    "                sampling_strategy = {exercice: min(100000, current_class_counts[exercice]) for exercice in EXERCICES_TYPE}\n",
    "                under_sampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42)\n",
    "                X_resampled, y_resampled = under_sampler.fit_resample(X, y)\n",
    "                print(f\"After under-sampling, class distribution: {pd.Series(y_resampled).value_counts()}\")\n",
    "                balanced_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                                    pd.DataFrame({'label': y_resampled}), \n",
    "                                    pd.DataFrame({'patient': z.iloc[:len(y_resampled)]})], axis=1)\n",
    "                # Sauvegarder le DataFrame rééchantillonné dans un nouveau fichier CSV\n",
    "                balanced_df.dropna(inplace=True)\n",
    "                df = balanced_df\n",
    "            else:\n",
    "                # Appliquer SMOTE en ajustant la stratégie de rééchantillonnage\n",
    "                sampling_strategy = {exercice: 100000 for exercice in EXERCICES_TYPE}\n",
    "                smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "                balanced_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                                    pd.DataFrame({'label': y_resampled}), \n",
    "                                    pd.DataFrame({'patient': z.iloc[:len(y_resampled)]})], axis=1)\n",
    "                # Sauvegarder le DataFrame rééchantillonné dans un nouveau fichier CSV\n",
    "                \n",
    "                df = balanced_df\n",
    "                print(f\"After SMOTE, class distribution: {pd.Series(y_resampled).value_counts()}\")\n",
    "\n",
    "            # Vérification de l'équilibre des classes après rééchantillonnage\n",
    "            if check_class_balance(y_resampled):\n",
    "                balanced_df = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), \n",
    "                                    pd.DataFrame({'label': y_resampled}), \n",
    "                                    pd.DataFrame({'patient': z})], axis=1)\n",
    "                balanced_df['patient']=balanced_df['patient'].apply(lambda x: np.random.randint(1, 26) if pd.isna(x) else x)\n",
    "                # Sauvegarder le DataFrame rééchantillonné dans un nouveau fichier CSV\n",
    "                balanced_df.to_csv(file_path, index=False)\n",
    "                print(f\"Successfully balanced and saved {file_path}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Warning: Classes are still not balanced in {file_path} after attempt {attempts + 1}\")\n",
    "                \n",
    "        else:\n",
    "            print(f\"Already balanced {file_path}\")\n",
    "            break\n",
    "\n",
    "        attempts += 1\n",
    "        if attempts >= max_attempts:\n",
    "            print(f\"Max attempts reached for {file_path}. Consider adjusting strategy or target count.\")\n",
    "\n",
    "for file_path in LOGO_SPLIT_FILES_PATH:\n",
    "    resample_and_check(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_measures_normal = pd.DataFrame(columns=list(EXERCICES_TYPE), index=list(SENSORS_TYPE.keys()))\n",
    "number_of_measures_logo = pd.DataFrame(columns=list(EXERCICES_TYPE), index=list(SENSORS_TYPE.keys()))\n",
    "\n",
    "for exercice in EXERCICES_TYPE:\n",
    "    for sensor in SENSORS_TYPE:\n",
    "        normal_data = pd.read_csv(f\"normal_split/{sensor}.csv\")\n",
    "        logo_data = pd.read_csv(f\"logo_split/{sensor}.csv\")\n",
    "        number_of_measures_normal.loc[sensor,exercice] = normal_data[normal_data['label'] == exercice].shape[0]\n",
    "        number_of_measures_logo.loc[sensor,exercice] = logo_data[logo_data['label'] == exercice].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 6))  # 1 ligne, 2 colonnes\n",
    "\n",
    "# Graphique 1: Mesures Normal\n",
    "number_of_measures_normal.plot(kind='bar', ax=ax1, width=0.8, color=['skyblue', 'lightgreen', 'coral', 'violet'])\n",
    "ax1.set_xlabel(\"Type de capteur\", fontsize=12)\n",
    "ax1.set_ylabel(\"Nombre de mesures\", fontsize=12)\n",
    "ax1.set_title(\"Répartition des données Normal Split\", fontsize=14)\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "# Add legend\n",
    "ax1.legend(title=\"Type d'exercice\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "# Graphique 2: Mesures LOGO\n",
    "number_of_measures_normal.plot(kind='bar', ax=ax2, width=0.8, color=['skyblue', 'lightgreen', 'coral', 'violet'])\n",
    "ax2.set_xlabel(\"Type de capteur\", fontsize=12)\n",
    "ax2.set_ylabel(\"Nombre de mesures\", fontsize=12)\n",
    "ax2.set_title(\"Répartition des données LOGO Split\", fontsize=14)\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "# Add legend\n",
    "ax2.legend(title=\"Type d'exercice\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "# Ajuster les espacements pour éviter le chevauchement\n",
    "plt.tight_layout()\n",
    "\n",
    "# Afficher les graphiques\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "for sensor in SENSORS_TYPE:\n",
    "    data = pd.read_csv(f\"normal_split/{sensor}.csv\")\n",
    "    combined_df.drop(columns=['label'], inplace=True,errors='ignore')\n",
    "    combined_df = pd.concat([combined_df, data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size, step_size):\n",
    "    \"\"\"\n",
    "    Crée des fenêtres glissantes à partir des données.\n",
    "    :param data: array-like, données à segmenter\n",
    "    :param window_size: int, taille de la fenêtre (en nombre d'échantillons)\n",
    "    :param step_size: int, pas de la fenêtre (en nombre d'échantillons)\n",
    "    :return: X (données segmentées), y (étiquettes correspondantes)\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    labels = []\n",
    "\n",
    "    # Découper les données en fenêtres glissantes\n",
    "    for start in range(0, len(data) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        window = data[start:end].drop('label', axis=1)  # Exclure la colonne 'label'\n",
    "        windows.append(window[:-1])\n",
    "\n",
    "        # L'étiquette correspond à la classe à partir du centre de la fenêtre\n",
    "        # Cela suppose que l'étiquette correspond à la fenêtre entière\n",
    "        labels.append(data.iloc[end - 1]['label'])  # Utiliser l'étiquette à la fin de la fenêtre\n",
    "    \n",
    "    # Convertir en arrays numpy pour Keras\n",
    "    X = np.array(windows)  # Exclure la dernière fenêtre pour éviter les problèmes de dimensions\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres de la fenêtre\n",
    "window_size = 60  # Taille de la fenêtre (60 échantillons pour 1 seconde)\n",
    "step_size = 60  # Pas de la fenêtre (60 échantillons pour pas de chevauchement)\n",
    "\n",
    "# Segmenter les données en fenêtres\n",
    "X, y = create_windows(combined_df, window_size, step_size)\n",
    "\n",
    "# Affichage des dimensions des données segmentées\n",
    "print(f'X shape: {X.shape}, y shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des données sur chaque fenêtre\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Appliquer la normalisation sur chaque fenêtre (chaque \"fenêtre\" dans X)\n",
    "X_scaled = np.array([scaler.fit_transform(window) for window in X])\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encodage des labels en one-hot\n",
    "y_encoded = to_categorical(label_encoder.fit_transform(y), num_classes=4)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour créer le modèle CNN-LSTM\n",
    "def create_cnn_lstm_model(input_shape, filters=32, kernel_size=3, lstm_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes à prédire\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Encapsuler le modèle dans KerasClassifier pour l'utiliser avec GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_cnn_lstm_model, input_shape=X_train.shape[1:], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Définir la grille des hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Résultats de la recherche\n",
    "print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle CNN-LSTM\n",
    "cnn_lstm_model = create_cnn_lstm_model(X_train.shape[1:3])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history_cnn_lstm_normal = cnn_lstm_model.fit(X_train, y_train, epochs=grid_search.best_params_['epochs'], batch_size=grid_search.best_params_['batch_size'], validation_data=(X_test, y_test))\n",
    "\n",
    "# Évaluer les performances sur l'ensemble de test\n",
    "score_cnn_lstm_normal = cnn_lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Score du modèle CNN-LSTM : {score_cnn_lstm_normal}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Paramètres de la fenêtre\n",
    "window_size = 60  # Taille de la fenêtre (60 échantillons pour 1 seconde)\n",
    "step_size = 60  # Pas de la fenêtre (60 échantillons pour pas de chevauchement)\n",
    "\n",
    "X_ips, y = create_windows(pd.read_csv(\"normal_split/ips.csv\"), window_size, step_size)\n",
    "X_emg, y = create_windows(pd.read_csv(\"normal_split/emg.csv\"), window_size, step_size)\n",
    "X_imu, y = create_windows(pd.read_csv(\"normal_split/imu.csv\"), window_size, step_size)\n",
    "X_cop, y = create_windows(pd.read_csv(\"normal_split/cop.csv\"), window_size, step_size)\n",
    "X_mocap, y = create_windows(pd.read_csv(\"normal_split/mocap.csv\"), window_size, step_size)\n",
    "\n",
    "# Normalisation pour chaque modalité (EMG, IMU, IPS)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normaliser les données EMG et IMU sur chaque fenêtre (chaque \"fenêtre\" dans X_emg et X_imu)\n",
    "X_emg_scaled = np.array([scaler.fit_transform(window) for window in X_emg])\n",
    "X_imu_scaled = np.array([scaler.fit_transform(window) for window in X_imu])\n",
    "X_ips_scaled = np.array([scaler.fit_transform(window) for window in X_ips])\n",
    "X_cop_scaled = np.array([scaler.fit_transform(window) for window in X_cop])\n",
    "X_mocap_scaled = np.array([scaler.fit_transform(window) for window in X_mocap])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_encoded = to_categorical(label_encoder.fit_transform(y), num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test (80% - 20%)\n",
    "X_emg_train, X_emg_test, X_imu_train, X_imu_test, X_cop_train,X_cop_test,X_mocap_train,X_mocap_test,X_ips_train, X_ips_test, y_train, y_test = train_test_split(\n",
    "    X_emg_scaled, X_imu_scaled,X_cop_scaled,X_mocap_scaled, X_ips_scaled, y_encoded, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = [\n",
    "    X_emg_train.shape[1:3],\n",
    "    X_imu_train.shape[1:3],\n",
    "    X_ips_train.shape[1:3],\n",
    "    X_cop_train.shape[1:3],\n",
    "    X_mocap_train.shape[1:3],\n",
    "]\n",
    "\n",
    "print(input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Architecture CNN-LSTM multimodale\n",
    "def create_cnn_lstm_multimodal_model(input_shapes):\n",
    "    # Sous-modèle EMG (ici on suppose que c'est une séquence temporelle, donc on utilise un CNN-LSTM)\n",
    "    emg_input = Input(shape=input_shapes[0], name='emg_input')\n",
    "    emg_conv1D = Conv1D(32, kernel_size=3, activation='relu')(emg_input)\n",
    "    emg_maxPooling1D = MaxPooling1D(pool_size=2)(emg_conv1D)\n",
    "    emg_dropout = Dropout(0.2)(emg_maxPooling1D)\n",
    "    emg_lstm = LSTM(64, activation='relu', return_sequences=False)(emg_dropout)\n",
    "    emg_dense = Dropout(0.2)(emg_lstm)\n",
    "    \n",
    "    # Sous-modèle IMU (si c'est aussi une séquence temporelle, appliquer CNN-LSTM ici aussi)\n",
    "    imu_input = Input(shape=input_shapes[1], name='imu_input')\n",
    "    imu_conv1D = Conv1D(32, kernel_size=3, activation='relu')(imu_input)\n",
    "    imu_maxPooling1D = MaxPooling1D(pool_size=2)(imu_conv1D)\n",
    "    imu_dropout = Dropout(0.2)(imu_maxPooling1D)\n",
    "    imu_lstm = LSTM(64, activation='relu', return_sequences=False)(imu_dropout)\n",
    "    imu_dense = Dropout(0.2)(imu_lstm)\n",
    "\n",
    "    # Sous-modèle IPS (données non séquentielles, on utilise des couches denses simples)\n",
    "    ips_input = Input(shape=input_shapes[2], name='ips_input')\n",
    "    ips_conv1D = Conv1D(32, kernel_size=3, activation='relu')(ips_input)\n",
    "    ips_maxPooling1D = MaxPooling1D(pool_size=2)(ips_conv1D)\n",
    "    ips_dropout = Dropout(0.2)(ips_maxPooling1D)\n",
    "    ips_lstm = LSTM(64, activation='relu', return_sequences=False)(ips_dropout)\n",
    "    ips_dense = Dropout(0.2)(ips_lstm)\n",
    "\n",
    "    # Sous-modèle COP\n",
    "    cop_input = Input(shape=input_shapes[3], name='cop_input')\n",
    "    cop_conv1D = Conv1D(32, kernel_size=3, activation='relu')(cop_input)\n",
    "    cop_maxPooling1D = MaxPooling1D(pool_size=2)(cop_conv1D)\n",
    "    cop_dropout = Dropout(0.2)(cop_maxPooling1D)\n",
    "    cop_lstm = LSTM(64, activation='relu', return_sequences=False)(cop_dropout)\n",
    "    cop_dense = Dropout(0.2)(cop_lstm)\n",
    "\n",
    "    # Sous-modèle MOCAP\n",
    "    mocap_input = Input(shape=input_shapes[4], name='mocap_input')\n",
    "    mocap_conv1D = Conv1D(32, kernel_size=3, activation='relu')(mocap_input)\n",
    "    mocap_maxPooling1D = MaxPooling1D(pool_size=2)(mocap_conv1D)\n",
    "    mocap_dropout = Dropout(0.2)(mocap_maxPooling1D)\n",
    "    mocap_lstm = LSTM(64, activation='relu', return_sequences=False)(mocap_dropout)\n",
    "    mocap_dense = Dropout(0.2)(mocap_lstm)\n",
    "\n",
    "    # Fusion des sorties (EMG, IMU, IPS, COP, MOCAP)\n",
    "    concatenated = Concatenate()([emg_dense, imu_dense, ips_dense, cop_dense, mocap_dense])\n",
    "\n",
    "    # Couche dense après fusion\n",
    "    dense = Dense(64, activation='relu')(concatenated)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "\n",
    "    # Couche de sortie\n",
    "    output = Dense(4, activation='softmax')(dense)  # 4 classes à prédire\n",
    "\n",
    "    # Création du modèle final\n",
    "    model = Model(inputs=[emg_input, imu_input, ips_input, cop_input, mocap_input], outputs=output)\n",
    "\n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_multimodal_model = create_cnn_lstm_multimodal_model(input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement du modèle multimodal CNN-LSTM\n",
    "history_cnn_lstm_multimodal_normal = cnn_lstm_multimodal_model.fit(\n",
    "    [X_emg_train, X_imu_train, X_ips_train,X_cop_train,X_mocap_train], y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=([X_emg_test, X_imu_test, X_ips_test,X_cop_test,X_mocap_test], y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cnn_lstm_multimodal_normal = cnn_lstm_multimodal_model.evaluate([X_emg_train, X_imu_train, X_ips_train,X_cop_train,X_mocap_train], y_train)\n",
    "print(f\"Score du modèle CNN-LSTM : {score_cnn_lstm_multimodal_normal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher le résumé du modèle\n",
    "cnn_lstm_multimodal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Code for CNN-LSTM ------------------------\n",
    "# Prédictions sur l'ensemble de test pour CNN-LSTM\n",
    "y_pred_cnn_lstm_normal = np.argmax(cnn_lstm_model.predict(X_test), axis=-1)\n",
    "\n",
    "# Si y_test est one-hot encodé, convertir y_test en labels d'indices entiers\n",
    "y_test_labels_normal = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Générer un rapport de classification pour CNN-LSTM\n",
    "report_cnn_lstm_normal = classification_report(y_test_labels_normal, y_pred_cnn_lstm_normal, target_names=label_encoder.classes_, output_dict=True)\n",
    "\n",
    "\n",
    "# Matrice de confusion pour CNN-LSTM\n",
    "conf_matrix_cnn_lstm_normal= confusion_matrix(y_test_labels_normal, y_pred_cnn_lstm_normal)\n",
    "\n",
    "# ------------------------ Code for Multimodal CNN-LSTM ------------------------\n",
    "# Prédictions sur l'ensemble de test pour le modèle multimodal CNN-LSTM\n",
    "predictions_multimodal_normal = cnn_lstm_multimodal_model.predict([X_emg_test, X_imu_test, X_ips_test, X_cop_test, X_mocap_test])\n",
    "\n",
    "y_pred_multimodal_normal = np.argmax(predictions_multimodal_normal, axis=-1)\n",
    "\n",
    "# Générer un rapport de classification pour le modèle multimodal CNN-LSTM\n",
    "report_multimodal_normal = classification_report(y_test_labels_normal, y_pred_multimodal_normal, target_names=label_encoder.classes_, output_dict=True)\n",
    "\n",
    "# Matrice de confusion pour le modèle multimodal CNN-LSTM\n",
    "conf_matrix_multimodal_normal = confusion_matrix(y_test_labels_normal, y_pred_multimodal_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.DataFrame()\n",
    "for sensor in SENSORS_TYPE:\n",
    "    data = pd.read_csv(f\"logo_split/{sensor}.csv\")\n",
    "    combined_df.drop(columns=['label'], inplace=True,errors='ignore')\n",
    "    combined_df.drop(columns=['patient'], inplace=True,errors='ignore')\n",
    "    combined_df = pd.concat([combined_df, data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size, step_size):\n",
    "    \"\"\"\n",
    "    Crée des fenêtres glissantes à partir des données.\n",
    "    :param data: DataFrame, données à segmenter\n",
    "    :param window_size: int, taille de la fenêtre (en nombre d'échantillons)\n",
    "    :param step_size: int, pas de la fenêtre (en nombre d'échantillons)\n",
    "    :return: X (données segmentées), y (étiquettes correspondantes)\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    labels = []\n",
    "\n",
    "    # Découper les données en fenêtres glissantes\n",
    "    for start in range(0, len(data) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        window = data[start:end].drop('label', axis=1)  # Exclure la colonne 'label'\n",
    "        windows.append(window[:-1])\n",
    "\n",
    "        # L'étiquette correspond à la classe à partir du centre de la fenêtre\n",
    "        labels.append(data.iloc[end - 1]['label'])  # Utiliser l'étiquette à la fin de la fenêtre\n",
    "    \n",
    "    # Convertir en arrays numpy pour Keras\n",
    "    X = np.array(windows)  # Exclure la dernière fenêtre pour éviter les problèmes de dimensions\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Paramètres de la fenêtre\n",
    "window_size = 60  # Taille de la fenêtre (60 échantillons pour 1 seconde)\n",
    "step_size = 60  # Pas de la fenêtre (60 échantillons pour pas de chevauchement)\n",
    "\n",
    "# Supposons que combined_df soit déjà votre DataFrame\n",
    "# Identifiez les 25 patients uniques\n",
    "patients = combined_df['patient'].unique()\n",
    "\n",
    "# Diviser les patients en ensembles d'entraînement et de test\n",
    "train_patients = patients[:20]  # 20 premiers patients pour l'entraînement\n",
    "test_patients = patients[-5:]   # 5 derniers patients pour le test\n",
    "\n",
    "# Filtrer les données en fonction des patients\n",
    "train_data = combined_df[combined_df['patient'].isin(train_patients)]\n",
    "test_data = combined_df[combined_df['patient'].isin(test_patients)]\n",
    "\n",
    "# Créer les fenêtres pour l'entraînement et le test\n",
    "X_train, y_train = create_windows(train_data, window_size, step_size)\n",
    "X_test, y_test = create_windows(test_data, window_size, step_size)\n",
    "\n",
    "# Normalisation des données sur chaque fenêtre\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Appliquer la normalisation sur chaque fenêtre (chaque \"fenêtre\" dans X)\n",
    "X_train = np.array([scaler.fit_transform(window) for window in X_train])\n",
    "X_test = np.array([scaler.transform(window) for window in X_test])  # Utiliser le même scaler pour le test\n",
    "\n",
    "# Encodage des labels en one-hot\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = to_categorical(label_encoder.fit_transform(y_train), num_classes=4)\n",
    "y_test = to_categorical(label_encoder.transform(y_test), num_classes=4)  # Utiliser le même LabelEncoder\n",
    "\n",
    "# Affichage des dimensions des données segmentées et normalisées\n",
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}, y_test shape: {y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour créer le modèle CNN-LSTM\n",
    "def create_cnn_lstm_model(input_shape, filters=32, kernel_size=3, lstm_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, kernel_size=kernel_size, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(lstm_units, activation='relu', return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(4, activation='softmax'))  # 4 classes à prédire\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Encapsuler le modèle dans KerasClassifier pour l'utiliser avec GridSearchCV\n",
    "model = KerasClassifier(build_fn=create_cnn_lstm_model, input_shape=X_train.shape[1:], epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# Définir la grille des hyperparamètres à tester\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Résultats de la recherche\n",
    "print(f\"Meilleurs paramètres: {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer le modèle CNN-LSTM\n",
    "cnn_lstm_model = create_cnn_lstm_model(X_train.shape[1:3])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history_cnn_lstm = cnn_lstm_model.fit(X_train, y_train, epochs=grid_search.best_params_['epochs'], batch_size=grid_search.best_params_['batch_size'], validation_data=(X_test, y_test))\n",
    "\n",
    "# Évaluer les performances sur l'ensemble de test\n",
    "score_cnn_lstm = cnn_lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Score du modèle CNN-LSTM : {score_cnn_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size, step_size):\n",
    "    \"\"\"\n",
    "    Crée des fenêtres glissantes à partir des données.\n",
    "    :param data: DataFrame, données à segmenter\n",
    "    :param window_size: int, taille de la fenêtre (en nombre d'échantillons)\n",
    "    :param step_size: int, pas de la fenêtre (en nombre d'échantillons)\n",
    "    :return: X (données segmentées), y (étiquettes correspondantes)\n",
    "    \"\"\"\n",
    "    windows = []\n",
    "    labels = []\n",
    "\n",
    "    # Découper les données en fenêtres glissantes\n",
    "    for start in range(0, len(data) - window_size, step_size):\n",
    "        end = start + window_size\n",
    "        window = data[start:end].drop('label', axis=1)  # Exclure la colonne 'label'\n",
    "        windows.append(window[:-1])\n",
    "\n",
    "        # L'étiquette correspond à la classe à partir du centre de la fenêtre\n",
    "        labels.append(data.iloc[end - 1]['label'])  # Utiliser l'étiquette à la fin de la fenêtre\n",
    "    \n",
    "    # Convertir en arrays numpy pour Keras\n",
    "    X = np.array(windows)  # Exclure la dernière fenêtre pour éviter les problèmes de dimensions\n",
    "    y = np.array(labels)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Paramètres de la fenêtre\n",
    "window_size = 60  # Taille de la fenêtre (60 échantillons pour 1 seconde)\n",
    "step_size = 60  # Pas de la fenêtre (60 échantillons pour pas de chevauchement)\n",
    "\n",
    "# Lire les fichiers de données pour chaque modalité\n",
    "ips_df = pd.read_csv(\"logo_split/ips.csv\")\n",
    "emg_df = pd.read_csv(\"logo_split/emg.csv\")\n",
    "imu_df = pd.read_csv(\"logo_split/imu.csv\")\n",
    "cop_df = pd.read_csv(\"logo_split/cop.csv\")\n",
    "mocap_df = pd.read_csv(\"logo_split/mocap.csv\")\n",
    "\n",
    "# Identifiez les 25 patients uniques (en supposant qu'il y a une colonne 'patient')\n",
    "patients = ips_df['patient'].unique()\n",
    "\n",
    "# Diviser les patients en ensembles d'entraînement et de test\n",
    "train_patients = patients[:20]  # 20 premiers patients pour l'entraînement\n",
    "test_patients = patients[-5:]   # 5 derniers patients pour le test\n",
    "\n",
    "# Filtrer les données en fonction des patients pour chaque modalité\n",
    "train_ips = ips_df[ips_df['patient'].isin(train_patients)]\n",
    "test_ips = ips_df[ips_df['patient'].isin(test_patients)]\n",
    "\n",
    "train_emg = emg_df[emg_df['patient'].isin(train_patients)]\n",
    "test_emg = emg_df[emg_df['patient'].isin(test_patients)]\n",
    "\n",
    "train_imu = imu_df[imu_df['patient'].isin(train_patients)]\n",
    "test_imu = imu_df[imu_df['patient'].isin(test_patients)]\n",
    "\n",
    "train_cop = cop_df[cop_df['patient'].isin(train_patients)]\n",
    "test_cop = cop_df[cop_df['patient'].isin(test_patients)]\n",
    "\n",
    "train_mocap = mocap_df[mocap_df['patient'].isin(train_patients)]\n",
    "test_mocap = mocap_df[mocap_df['patient'].isin(test_patients)]\n",
    "\n",
    "# Créer les fenêtres pour chaque modalité\n",
    "X_ips_train, y_train = create_windows(train_ips, window_size, step_size)\n",
    "X_emg_train, y_train = create_windows(train_emg, window_size, step_size)\n",
    "X_imu_train, y_train = create_windows(train_imu, window_size, step_size)\n",
    "X_cop_train, y_train = create_windows(train_cop, window_size, step_size)\n",
    "X_mocap_train, y_train = create_windows(train_mocap, window_size, step_size)\n",
    "\n",
    "X_ips_test, y_test = create_windows(test_ips, window_size, step_size)\n",
    "X_emg_test, y_test = create_windows(test_emg, window_size, step_size)\n",
    "X_imu_test, y_test = create_windows(test_imu, window_size, step_size)\n",
    "X_cop_test, y_test = create_windows(test_cop, window_size, step_size)\n",
    "X_mocap_test, y_test = create_windows(test_mocap, window_size, step_size)\n",
    "\n",
    "# Normalisation des données sur chaque fenêtre pour chaque modalité\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Normaliser les données pour chaque modalité (EMG, IMU, IPS, COP, Mocap)\n",
    "# Normaliser les données d'entraînement\n",
    "X_ips_train = np.array([scaler.fit_transform(window) for window in X_ips_train])\n",
    "X_ips_test = np.array([scaler.transform(window) for window in X_ips_test])\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_emg_train = np.array([scaler.fit_transform(window) for window in X_emg_train])\n",
    "X_emg_test = np.array([scaler.transform(window) for window in X_emg_test])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_imu_train = np.array([scaler.fit_transform(window) for window in X_imu_train])\n",
    "X_imu_test = np.array([scaler.transform(window) for window in X_imu_test])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_cop_train = np.array([scaler.fit_transform(window) for window in X_cop_train])\n",
    "X_cop_test = np.array([scaler.transform(window) for window in X_cop_test])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_mocap_train = np.array([scaler.fit_transform(window) for window in X_mocap_train])\n",
    "X_mocap_test = np.array([scaler.transform(window) for window in X_mocap_test])\n",
    "\n",
    "# Encodage des labels en one-hot pour chaque modalité\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = to_categorical(label_encoder.fit_transform(y_train), num_classes=4)\n",
    "y_test = to_categorical(label_encoder.transform(y_test), num_classes=4)\n",
    "\n",
    "# Affichage des dimensions des données segmentées et normalisées pour chaque modalité\n",
    "print(f'X_ips_train shape: {X_ips_train.shape}, X_ips_test shape: {X_ips_test.shape}')\n",
    "print(f'X_emg_train shape: {X_emg_train.shape}, X_emg_test shape: {X_emg_test.shape}')\n",
    "print(f'X_imu_train shape: {X_imu_train.shape}, X_imu_test shape: {X_imu_test.shape}')\n",
    "print(f'X_cop_train shape: {X_cop_train.shape}, X_cop_test shape: {X_cop_test.shape}')\n",
    "print(f'X_mocap_train shape: {X_mocap_train.shape}, X_mocap_test shape: {X_mocap_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = [\n",
    "    X_emg_train.shape[1:3],\n",
    "    X_imu_train.shape[1:3],\n",
    "    X_ips_train.shape[1:3],\n",
    "    X_cop_train.shape[1:3],\n",
    "    X_mocap_train.shape[1:3],\n",
    "]\n",
    "\n",
    "print(input_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture CNN-LSTM multimodale\n",
    "def create_cnn_lstm_multimodal_model(input_shapes):\n",
    "    # Sous-modèle EMG (ici on suppose que c'est une séquence temporelle, donc on utilise un CNN-LSTM)\n",
    "    emg_input = Input(shape=input_shapes[0], name='emg_input')\n",
    "    emg_conv1D = Conv1D(32, kernel_size=3, activation='relu')(emg_input)\n",
    "    emg_maxPooling1D = MaxPooling1D(pool_size=2)(emg_conv1D)\n",
    "    emg_dropout = Dropout(0.2)(emg_maxPooling1D)\n",
    "    emg_lstm = LSTM(64, activation='relu', return_sequences=False)(emg_dropout)\n",
    "    emg_dense = Dropout(0.2)(emg_lstm)\n",
    "    \n",
    "    # Sous-modèle IMU (si c'est aussi une séquence temporelle, appliquer CNN-LSTM ici aussi)\n",
    "    imu_input = Input(shape=input_shapes[1], name='imu_input')\n",
    "    imu_conv1D = Conv1D(32, kernel_size=3, activation='relu')(imu_input)\n",
    "    imu_maxPooling1D = MaxPooling1D(pool_size=2)(imu_conv1D)\n",
    "    imu_dropout = Dropout(0.2)(imu_maxPooling1D)\n",
    "    imu_lstm = LSTM(64, activation='relu', return_sequences=False)(imu_dropout)\n",
    "    imu_dense = Dropout(0.2)(imu_lstm)\n",
    "\n",
    "    # Sous-modèle IPS (données non séquentielles, on utilise des couches denses simples)\n",
    "    ips_input = Input(shape=input_shapes[2], name='ips_input')\n",
    "    ips_conv1D = Conv1D(32, kernel_size=3, activation='relu')(ips_input)\n",
    "    ips_maxPooling1D = MaxPooling1D(pool_size=2)(ips_conv1D)\n",
    "    ips_dropout = Dropout(0.2)(ips_maxPooling1D)\n",
    "    ips_lstm = LSTM(64, activation='relu', return_sequences=False)(ips_dropout)\n",
    "    ips_dense = Dropout(0.2)(ips_lstm)\n",
    "\n",
    "    # Sous-modèle COP\n",
    "    cop_input = Input(shape=input_shapes[3], name='cop_input')\n",
    "    cop_conv1D = Conv1D(32, kernel_size=3, activation='relu')(cop_input)\n",
    "    cop_maxPooling1D = MaxPooling1D(pool_size=2)(cop_conv1D)\n",
    "    cop_dropout = Dropout(0.2)(cop_maxPooling1D)\n",
    "    cop_lstm = LSTM(64, activation='relu', return_sequences=False)(cop_dropout)\n",
    "    cop_dense = Dropout(0.2)(cop_lstm)\n",
    "\n",
    "    # Sous-modèle MOCAP\n",
    "    mocap_input = Input(shape=input_shapes[4], name='mocap_input')\n",
    "    mocap_conv1D = Conv1D(32, kernel_size=3, activation='relu')(mocap_input)\n",
    "    mocap_maxPooling1D = MaxPooling1D(pool_size=2)(mocap_conv1D)\n",
    "    mocap_dropout = Dropout(0.2)(mocap_maxPooling1D)\n",
    "    mocap_lstm = LSTM(64, activation='relu', return_sequences=False)(mocap_dropout)\n",
    "    mocap_dense = Dropout(0.2)(mocap_lstm)\n",
    "\n",
    "    # Fusion des sorties (EMG, IMU, IPS, COP, MOCAP)\n",
    "    concatenated = Concatenate()([emg_dense, imu_dense, ips_dense, cop_dense, mocap_dense])\n",
    "\n",
    "    # Couche dense après fusion\n",
    "    dense = Dense(64, activation='relu')(concatenated)\n",
    "    dense = Dropout(0.2)(dense)\n",
    "\n",
    "    # Couche de sortie\n",
    "    output = Dense(4, activation='softmax')(dense)  # 4 classes à prédire\n",
    "\n",
    "    # Création du modèle final\n",
    "    model = Model(inputs=[emg_input, imu_input, ips_input, cop_input, mocap_input], outputs=output)\n",
    "\n",
    "    # Compilation du modèle\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "cnn_lstm_multimodal_model = create_cnn_lstm_multimodal_model(input_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_value_train = min(X_emg_train.shape[0], X_imu_train.shape[0], X_ips_train.shape[0], X_cop_train.shape[0], X_mocap_train.shape[0])\n",
    "min_value_test = min(X_emg_test.shape[0], X_imu_test.shape[0], X_ips_test.shape[0], X_cop_test.shape[0], X_mocap_test.shape[0])\n",
    "\n",
    "X_emg_train = X_emg_train[:min_value_train]\n",
    "X_imu_train = X_imu_train[:min_value_train]\n",
    "X_ips_train = X_ips_train[:min_value_train]\n",
    "X_cop_train = X_cop_train[:min_value_train]\n",
    "X_mocap_train = X_mocap_train[:min_value_train]\n",
    "\n",
    "# Similarly for the test data\n",
    "X_emg_test = X_emg_test[:min_value_test]\n",
    "X_imu_test = X_imu_test[:min_value_test]\n",
    "X_ips_test = X_ips_test[:min_value_test]\n",
    "X_cop_test = X_cop_test[:min_value_test]\n",
    "X_mocap_test = X_mocap_test[:min_value_test]\n",
    "\n",
    "# Ensure that y_train has the same number of samples\n",
    "y_train = y_train[:min_value_train]\n",
    "y_test = y_test[:min_value_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_lstm_multimodal = cnn_lstm_multimodal_model.fit(\n",
    "    [X_emg_train, X_imu_train, X_ips_train,X_cop_train,X_mocap_train], y_train,\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    "    validation_data=([X_emg_test, X_imu_test, X_ips_test,X_cop_test,X_mocap_test], y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer les performances sur l'ensemble de test\n",
    "score_cnn_lstm_multimodal = cnn_lstm_multimodal_model.evaluate([X_emg_train, X_imu_train, X_ips_train,X_cop_train,X_mocap_train], y_train)\n",
    "print(f\"Score du modèle CNN-LSTM : {score_cnn_lstm_multimodal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Afficher le résumé du modèle\n",
    "cnn_lstm_multimodal_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Code for CNN-LSTM ------------------------\n",
    "# Prédictions sur l'ensemble de test pour CNN-LSTM\n",
    "y_pred_cnn_lstm = np.argmax(cnn_lstm_model.predict(X_test), axis=-1)\n",
    "\n",
    "# Si y_test est one-hot encodé, convertir y_test en labels d'indices entiers\n",
    "y_test_labels = np.argmax(y_test, axis=-1)\n",
    "\n",
    "y_test_labels = y_test_labels[:min(y_test_labels.shape[0], y_pred_cnn_lstm.shape[0])]\n",
    "\n",
    "y_pred_cnn_lstm = y_pred_cnn_lstm[:min(y_test_labels.shape[0], y_pred_cnn_lstm.shape[0])]\n",
    "\n",
    "# Générer un rapport de classification pour CNN-LSTM\n",
    "report_cnn_lstm = classification_report(y_test_labels, y_pred_cnn_lstm, target_names=label_encoder.classes_,output_dict=True)\n",
    "\n",
    "# Matrice de confusion pour CNN-LSTM\n",
    "conf_matrix_cnn_lstm = confusion_matrix(y_test_labels, y_pred_cnn_lstm)\n",
    "\n",
    "# ------------------------ Code for Multimodal CNN-LSTM ------------------------\n",
    "# Prédictions sur l'ensemble de test pour le modèle multimodal CNN-LSTM\n",
    "predictions_multimodal = cnn_lstm_multimodal_model.predict([X_emg_test, X_imu_test, X_ips_test, X_cop_test, X_mocap_test])\n",
    "\n",
    "y_pred_multimodal = np.argmax(predictions_multimodal, axis=-1)\n",
    "\n",
    "y_test_labels = y_test_labels[:min_value_test]\n",
    "\n",
    "# Générer un rapport de classification pour le modèle multimodal CNN-LSTM\n",
    "report_multimodal = classification_report(y_test_labels, y_pred_multimodal, target_names=label_encoder.classes_, output_dict=True)\n",
    "print(\"Rapport de Classification - Multimodal CNN-LSTM :\\n\", report_multimodal)\n",
    "\n",
    "# Matrice de confusion pour le modèle multimodal CNN-LSTM\n",
    "conf_matrix_multimodal = confusion_matrix(y_test_labels, y_pred_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Plotting Results ------------------------\n",
    "\n",
    "# Courbes d'apprentissage pour les deux modèles\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Courbes d'apprentissage - CNN-LSTM\n",
    "plt.subplot(4, 2, 1)\n",
    "plt.plot(history_cnn_lstm_normal.history['accuracy'], label='Précision d\\'entraînement')\n",
    "plt.plot(history_cnn_lstm_normal.history['val_accuracy'], label='Précision de validation')\n",
    "plt.title('Courbe d\\'apprentissage - CNN-LSTM - Normal')\n",
    "plt.xlabel('Épochs')\n",
    "plt.ylabel('Précision')\n",
    "plt.legend()\n",
    "\n",
    "# Matrice de confusion - CNN-LSTM\n",
    "plt.subplot(4, 2, 2)\n",
    "sns.heatmap(conf_matrix_cnn_lstm_normal, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Vraies étiquettes')\n",
    "plt.title('Matrice de Confusion - CNN-LSTM - Normal')\n",
    "\n",
    "# Courbes d'apprentissage - Multimodal CNN-LSTM\n",
    "plt.subplot(4, 2, 3)\n",
    "plt.plot(history_cnn_lstm_multimodal_normal.history['accuracy'], label='Précision d\\'entraînement')\n",
    "plt.plot(history_cnn_lstm_multimodal_normal.history['val_accuracy'], label='Précision de validation')\n",
    "plt.title('Courbe d\\'apprentissage - Multimodal CNN-LSTM - Normal')\n",
    "plt.xlabel('Épochs')\n",
    "plt.ylabel('Précision')\n",
    "plt.legend()\n",
    "\n",
    "# Matrice de confusion - Multimodal CNN-LSTM\n",
    "plt.subplot(4, 2, 4)\n",
    "sns.heatmap(conf_matrix_multimodal_normal, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Vraies étiquettes')\n",
    "plt.title('Matrice de Confusion - Multimodal CNN-LSTM - Normal')\n",
    "\n",
    "# ------------------------ Plotting Results ------------------------\n",
    "\n",
    "# Courbes d'apprentissage - CNN-LSTM\n",
    "plt.subplot(4, 2, 5)\n",
    "plt.plot(history_cnn_lstm.history['accuracy'], label='Précision d\\'entraînement')\n",
    "plt.plot(history_cnn_lstm.history['val_accuracy'], label='Précision de validation')\n",
    "plt.title('Courbe d\\'apprentissage - CNN-LSTM - LOGO')\n",
    "plt.xlabel('Épochs')\n",
    "plt.ylabel('Précision')\n",
    "plt.legend()\n",
    "\n",
    "# Matrice de confusion - CNN-LSTM\n",
    "plt.subplot(4, 2, 6)\n",
    "sns.heatmap(conf_matrix_cnn_lstm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Vraies étiquettes')\n",
    "plt.title('Matrice de Confusion - CNN-LSTM - LOGO')\n",
    "\n",
    "# Courbes d'apprentissage - Multimodal CNN-LSTM\n",
    "plt.subplot(4, 2, 7)\n",
    "plt.plot(history_cnn_lstm_multimodal.history['accuracy'], label='Précision d\\'entraînement')\n",
    "plt.plot(history_cnn_lstm_multimodal.history['val_accuracy'], label='Précision de validation')\n",
    "plt.title('Courbe d\\'apprentissage - Multimodal CNN-LSTM - LOGO')\n",
    "plt.xlabel('Épochs')\n",
    "plt.ylabel('Précision')\n",
    "plt.legend()\n",
    "\n",
    "# Matrice de confusion - Multimodal CNN-LSTM\n",
    "plt.subplot(4, 2, 8)\n",
    "sns.heatmap(conf_matrix_multimodal, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Prédictions')\n",
    "plt.ylabel('Vraies étiquettes')\n",
    "plt.title('Matrice de Confusion - Multimodal CNN-LSTM - LOGO')\n",
    "\n",
    "# Ajustement pour éviter le chevauchement des titres et des étiquettes\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rapport de Classification - Multimodal CNN-LSTM - Normal :\\n\")\n",
    "pd.DataFrame(report_multimodal_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rapport de Classification - CNN-LSTM - Normal:\\n\")\n",
    "pd.DataFrame(report_cnn_lstm_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Rapport de Classification - Multimodal CNN-LSTM - LOGO :\\n\")\n",
    "\n",
    "pd.DataFrame(report_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rapport de Classification - CNN-LSTM - LOGO:\\n\")\n",
    "pd.DataFrame(report_cnn_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultats\n",
    "Les résultats montrent que :\n",
    "- Le modèle multimodal surpasse le modèle simple dans les deux stratégies de validation\n",
    "- La validation LOGO donne des résultats plus réalistes mais moins performants\n",
    "- Les performances sont meilleures sur le split normal, suggérant une possible sur-adaptation aux patterns individuels\n",
    "\n",
    "## Conclusion\n",
    "Cette étude démontre l'intérêt d'une approche multimodale pour la classification d'exercices, tout en soulignant l'importance d'une validation rigoureuse pour évaluer la généralisation des modèles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
